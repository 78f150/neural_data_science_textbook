

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <title>Open Science &#8212; NESC 3505 Neural Data Science</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/mystnb.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .secondtoggle, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed.js"></script>
    <link rel="canonical" href="https://dalpsychneuro.github.io/NESC_3505_textbook/1/open_science.html" />
    <link rel="shortcut icon" href="../_static/NESC3505_favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Open Educational Resources" href="oer.html" />
    <link rel="prev" title="Free Software" href="software.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">
<!-- Put requirejs at the end so it's always after bootstrap -->
<!-- TODO: remove this once https://github.com/pandas-dev/pydata-sphinx-theme/pull/149 is merged -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>


<!-- Opengraph tags -->
<meta property="og:url"         content="https://dalpsychneuro.github.io/NESC_3505_textbook/1/open_science.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Open Science" />
<meta property="og:description" content="Open Science  Open science is a movement to make the practice of scientific research, and dissemination of its results, freely accessible. In the present day, a" />
<meta property="og:image"       content="https://dalpsychneuro.github.io/NESC_3505_textbook/_static/NESC3505_icon.png" />

<meta name="twitter:card" content="summary">


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          

<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/NESC3505_icon.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">NESC 3505 Neural Data Science</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  
  <ul class="nav sidenav_l1">
  <li class="">
    <a href="../intro.html">Welcome</a>
  </li>
  <li class="">
    <a href="../syllabus.html">Syllabus</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">About This Course</p>
</li>
  <li class="">
    <a href="why.html">Start with why</a>
  </li>
  <li class="active">
    <a href="teaching_approach.html">Teaching Approach</a>
  <ul class="nav sidenav_l2">
    <li class="">
      <a href="online_learning.html">Online Learning</a>
    </li>
    <li class="">
      <a href="assessment_evaluation.html">Assessment and Evaluation</a>
    </li>
    <li class="">
      <a href="constructivism.html">Constructivism</a>
    </li>
    <li class="">
      <a href="connectivism.html">Connectivism</a>
    </li>
    <li class="">
      <a href="authenticity.html">Authenticity</a>
    </li>
    <li class="">
      <a href="21st_cen_skills.html">21st Century Skills</a>
    </li>
    <li class="active">
      <a href="open.html">Open Resources</a>
      <ul class="nav sidenav_l3">
      <li class="">
        <a href="software.html">Free Software</a>
      </li>
      <li class="active">
        <a href="">Open Science</a>
      </li>
      <li class="">
        <a href="oer.html">Open Educational Resources</a>
      </li>
      <li class="">
        <a href="licenses.html">Licenses</a>
      </li>
    </ul>
    </li>
  </ul>
  </li>
  <li class="">
    <a href="learning_approach.html">How To Rock This Course</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Getting Started</p>
</li>
  <li class="">
    <a href="../2/learning_objectives.html">Learning Objectives</a>
  </li>
  <li class="">
    <a href="../2/what_is_nds.html">What is data science?</a>
  </li>
  <li class="">
    <a href="../2/tools_for_nds.html">Tools for Neural Data Science</a>
  </li>
  <li class="">
    <a href="../2/class_tech.html">Technologies Used in this Class</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">Introducing Python</p>
</li>
  <li class="">
    <a href="../3/lesson1.html">Lesson 1: Introduction to Python</a>
  </li>
<li class="navbar-special">
<p class="margin-caption">References</p>
</li>
  <li class="">
    <a href="../zrefs.html">References</a>
  </li>
</ul>
</nav>
<p class="navbar_footer">Powered by <a href="https://jupyterbook.org">Jupyter Book</a></p>
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse" data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu" aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation" title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        <div class="dropdown-buttons-trigger">
            <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i class="fas fa-download"></i></button>

            
            <div class="dropdown-buttons">
                <!-- ipynb file if we had a myst markdown file -->
                
                <!-- Download raw file -->
                <a class="dropdown-buttons" href="../_sources/1/open_science.md.txt"><button type="button" class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip" data-placement="left">.md</button></a>
                <!-- Download PDF via print -->
                <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF" onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
            </div>
            
        </div>

        <!-- Edit this page -->
        

        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->
        

        
    </div>
    <div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#peer-review" class="nav-link">Peer Review</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#for-profit-publishing" class="nav-link">For-Profit Publishing</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#impact-factors" class="nav-link">Impact Factors</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#novelty" class="nav-link">Novelty</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#open-publishing" class="nav-link">Open Publishing</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#preregistration" class="nav-link">Preregistration</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#open-methods-and-data" class="nav-link">Open Methods and Data</a>
        </li>
    
    </ul>
</nav>


    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 col-xxl-7 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="open-science">
<h1>Open Science<a class="headerlink" href="#open-science" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Open_science">Open science</a> is a movement to make the practice of scientific research, and dissemination of its results, freely accessible. In the present day, as a student you may find that much of the scientific literature is readily accessible, in the form of journal articles made available through your university’s library. However, it is important to realize that access to most of those journals is available only because the university’s library pays a subscription fee to the journal publishers. Without a university affiliation, you would have to pay a (usually substantial) fee to access any article.</p>
<p>The open science movement addresses more than just scientific publishing, however. It also covers scientific practice, including openness and transparency in methodology, and free and open data sharing. Below we cover dissemination and methodology in more detail. The following section covers <a class="reference internal" href="oer.html"><span class="doc">Open Educational Resources</span></a>, which are also considered part of the open science movement, insofar as education is necessary for good scientific practice.</p>
<div class="section" id="peer-review">
<h2>Peer Review<a class="headerlink" href="#peer-review" title="Permalink to this headline">¶</a></h2>
<p>As a science student, you have no doubt learned that the “gold standard” of scientific evidence is empirical research published in peer-reviewed journals. Peer review is a hallmark of modern scientific practice and credibility, whereby scientists review each others’ work. This is typically done in a “blinded” fashion, whereby the authors of a paper, at least, do not know who the reviewers of the paper are (sometimes reviews are double-blinded, such that reviewers do not see the names or affiliations of the authors). As well, reviewers must declare that they have no conflict of interest in reviewing the work, such as a personal or professional relationship with the authors, or a financial incentive to recommend or prevent publication of the work. Peer reviewers are also not paid to perform this duty, but the vast majority of scientists engage in peer review for journals through a sense of collective duty; firstly, these scientists rely on their peers to review their own work, and secondly, peer review allows scientists to maintain quality control over their discipline. An added bonus is that reviewers get to see the latest scientific developments before others do.</p>
<p>While the principles of the peer review process are sound, they are not without problems. Firstly, the process typically involves an editor for the journal receiving a manuscript, and sending it out for peer review. Many papers are only reviewed by one or two peers, though this may go up to 5 or 6, but rarely more. Thus the people who review a paper may not be truly representative of the views of the wider scientific community. Furthermore, since peer review is unpaid labour, editors must rely on the good will — and availability — of reviewers. Editors also often do not know the people they invite as reviewers well, if at all, and so rely on a general principles of honesty and good faith (e.g., that a reviewer has appropriate expertise, and reviews with a neutral perspective). This process is thus prone to selection biases, whereby the people who provide reviews may neither be ideally-suited for the job, nor entirely neutral. Indeed, there are numerous cases of papers being rejected by reviewers who clearly were incentivized to do so by a desire to publish similar work first, or even steal ideas and use them as their own.</p>
<p>These issues have led to the development of <strong>open peer review</strong>. In this process, the names of peer reviewers — and their reviews — are published along with the journal article itself. Typically the names of reviewers are only revealed to the authors, and the public, if the article is accepted for publication. Some scientists choose to “sign” their reviews with their names as a matter of principle. In some cases, <a class="reference external" href="http://bradlove.org/blog/open-review">controversially</a>, reviewers have chosen to openly publish reviews of papers that they have recommended for rejection (<a class="reference external" href="https://www.pygaze.org/2019/01/what-the-kanye-west-vs-taylor-swift-of-neuroscience-tells-us-about-open-science/">see also this commentary</a>).</p>
</div>
<div class="section" id="for-profit-publishing">
<h2>For-Profit Publishing<a class="headerlink" href="#for-profit-publishing" title="Permalink to this headline">¶</a></h2>
<p>Distinct from transparency of the peer review process itself, is that of the fact that most scientific journals are published by for-profit companies. For-profit journal publishers have a unique monopoly, because any scientific article is only ever published in one journal. Thus to support the intellectual activities of professors, students, and staff, university libraries have little choice to pay whatever subscription fee the journal chooses to charge. Presently, these subscription costs comprise the vast majority of most university library budgets. For example, in the 2018-19 academic year, <a class="reference external" href="https://www.frdr-dfdr.ca/repo/handle/doi:10.20383/101.0187">Dalhousie University paid a total of over CAD$3.1 million for journal subscriptions</a>; nearly half of this was to one publisher, Elsevier/Science Direct. The cost of journal access has been recognized as a problem for universities across Canada and around the world; for example the Canadian Association of Research Libraries has described these costs as <a class="reference external" href="http://www.carl-abrc.ca/wp-content/uploads/2018/02/CARL_Brief_Subscription_Costs_en.pdf">“unsustainable”</a>. While journals certainly incur costs associated with the publishing of scientific articles, that same report notes that for-profit publishers typically have profit margins of 30% or greater, meaning that roughly 1/3 of subscription costs go to making the owners and shareholders of those companies more wealthy, rather than disseminating knowledge.</p>
<p>Adding insult to injury, in many people’s view, is the fact that this for-profit publishing industry relies on the good will, and free labour, of scientists. As noted, peer reviewers are not paid for this duty, and neither are journal editors (editorial roles do carry some prestige, which provides some additional benefit and incentive). So at the end of the day, publishers rely on the free labour of scientists at universities, but then charge those same universities high fees to access the work that their employees reviewed and edited for free. Moreover, most of the published scientific work was only possible due to research grants that covered the costs of the work — and these grants are primarily provided by governments, using money collected from taxes. In other words, the general public pays for research through their taxes, but is not able to access the results of the research because it is behind a paywall that, at best, only academic “elite” can access.</p>
</div>
<div class="section" id="impact-factors">
<h2>Impact Factors<a class="headerlink" href="#impact-factors" title="Permalink to this headline">¶</a></h2>
<p>Another problem that many scientists have with for-profit publishing is that individual journals are typically concerned with their reputation and, specifically, their <strong>impact factor</strong>. This is a score that reflects the ratio of the number of times papers in that journal are cited, relative to the total number of papers the journal has published. More citations of an article thus reflect more impact. Scientists often consider journal impact factor when deciding where to submit their work, both because of the prestige associated with high-impact journals, and ultimately because important career-related decisions (such as hiring, promotion, and grant awards), are often based on citations. A popular metric for evaluating the impact of a scientist is the <strong>h-index</strong>, which reflects the number of times their work has been cited (technically, <em>h</em> is defined such that the author has published <em>h</em> papers that have each been cited <em>h</em> times). Thus scientists anticipate that if their work is accepted by a high-impact factor journal, ultimately it will improve their <em>h</em>-index.</p>
<p>The concerning result of chasing impact factors is that journals will base decisions about which papers to accept (or even send out for review) not only on the scientific merit of the research, but on its anticipated impact. For example, as a reviewer I have received instruction from journals such as that the paper should be “ranked in the top 15-20% in terms of significance, originality, and design and quality of data”. While beneficial for journal impact factors, this exerts problematic pressure on scientists. Firstly, decisions are made based on the <em>anticipated</em> impact of a paper, by a relatively small number of people — which may or may not accurately reflect the actual impact the paper has when published. Secondly, it can lead to long delays in solid scientific work being published, if one or more journals review the paper and decide it doesn’t have sufficient potential impact, independent of its scientific rigour. Reviewers are typically given 2–4 weeks to provide a review, but are often late submitting reviews. Combined with the time it takes for an editor to find reviewers, this can mean that receiving a rejection takes a month or more (or much more, if the paper goes through more than one round of reviews before a reject decision is made). Combined with the authors finding the time to revise, reformat, and resubmit a paper to a new journal, this can mean that publication takes many months or even years after the research is completed.</p>
</div>
<div class="section" id="novelty">
<h2>Novelty<a class="headerlink" href="#novelty" title="Permalink to this headline">¶</a></h2>
<p>Related, but distinct, from the issue of impact factors is that most scientific journals have “novelty” as a publication criterion. In other words, a paper that presents a replication of previously-published work is unpublishable in the vast majority of journals. This is a serious problem that has led to a <strong>replication crisis</strong> in many fields, including Psychology. The replication crisis is, simply put, that many published results cannot be replicated. But, this information is often not available to the scientific community, because results published in peer-reviewed journals are considered the gold standard of “truth”. In fact, by one line of argument (and the title of a now-famous paper), the results of “most published research findings are false” <a class="reference external" href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124">Ioannidis, 2005</a>. Ioannidis’ argument demonstrates that chasing impact factors incentivizes journals and authors to publish results that are novel and surprising, but in general if a result is surprising, that is because it goes against what is already known. While in some cases these may be true results, the odds of this are lower than for confirmatory findings. Moreover, once such a result is published, it is both harder for anyone else to publish a replication of the methods, and even harder if the replication findings are not consistent with the published, peer-reviewed results. This is not to say that the published results in question are fraudulent, or that the authors had any ill intent in publishing them. Most scientific results are based on statistics, which rely on samples of a much larger population and estimates of the probability that the result is due to chance. This carries inherent risk that any particular result <em>is</em> due to chance, and the only real way to determine this is to replicate the experiment, ideally more than once. However, given the time and cost of research, as well as the dependence of scientific careers on publication (especially high-impact publication), scientists are much more likely to publish a result sooner, rather than wait for replication (if they can even afford to, either financially or career-wise).</p>
<p>Another practice that has contributed to the replicability crisis is behaviour by scientists that has been widely condoned in the past, but that is also questionable upon reflection. One such behaviour is <a class="reference external" href="https://doi.org/10.1207/s15327957pspr0203_4"><strong>HARKing</strong></a> <a class="bibtex reference internal" href="../zrefs.html#kerr-harking-1998" id="id1">[Ker98]</a> or hypothesizing after the results are known. This essentially entails analyzing data and then writing the results up with a hypothesis (prediction about the outcome of the experiment) that is based on the results, rather than one that was created <em>a priori</em>, before the study was conducted. Statistical testing is often described as hypothesis testing, because the interpretation of statistical results relies on estimates that a result could be obtained by chance. If the statistical result is consistent with an a priori theory, this provides additional reassurance that the result is correct — because it is consistent with a theoretical prediction. On the other hand (as noted above), surprising research results are also more likely to be false, such as the result of a Type 1 (false positive) error. Moreover, scientific hypotheses should be disconfirmable, but it is impossible to disconfirm a result that you only “predict” after you have found it (without doing more experiments). This is not to say there is no value in following up on unexpected results, or conducting exploratory research. However, it is critical to distinguish between results that you genuinely predicted before conducting the experiment, and those obtained <em>post hoc</em>.</p>
<p>Another questionable practice is <a class="reference external" href="https://doi.org/10.1371/journal.pbio.1002106"><strong>p-hacking</strong></a><a class="bibtex reference internal" href="../zrefs.html#phacking" id="id2">[HHL+15]</a>. This is the practice of iteratively collecting and analyzing data, until a specific, desired result is obtained, and then terminating data collection. This practice is problematic because, again, <em>p</em> values used to determine statistical significance reflect probability, not ultimate truth, and so there is always a probability of error. A properly-designed study will conduct a <strong>power analysis</strong> prior to starting, which involves using the expected size of the experimental effect, and variance among samples, to determine how large a sample size (e.g., number of participants) to collect. If the estimates of effect size and variance are accurate, then obtaining a significant <em>p</em> value from the results of a study using the estimated sample size can be taken to be probably true. However, p-hacking virtually guarantees false results, because one is essentially playing the lottery until one gets a “big win”, and then quitting. This is fine (and probably advisable) when playing roulette in Las Vegas, but it is frankly dishonest scientific practice.</p>
</div>
<div class="section" id="open-publishing">
<h2>Open Publishing<a class="headerlink" href="#open-publishing" title="Permalink to this headline">¶</a></h2>
<p>The open publishing movement aims to address the issues and concerns around for-profit publishing, by making peer-reviewed journal articles free for anyone to access. Prominent examples of open publishing are the <a class="reference external" href="https://plos.org">Public Library of Science</a>, <a class="reference external" href="https://www.frontiersin.org/">Frontiers</a>, and <a class="reference external" href="https://elifesciences.org/">eLife</a>. These journals are published by not-for-profit corporations, so, while reviewers and editors still perform their duties for free, they are not contributing to revenue-generating activities for a for-profit company. Since the publishing process still incurs significant costs (such as copyediting, final production, and web hosting), open journals cannot operate in a fully cost-free fashion. However, rather than charging for access to read the published works, these journals charge authors a publication fee. The logic behind this is that the authors hold research grants that pay for other aspects of conducting the research, so it is natural that the costs of disseminating the work also come from these grants. It is also important to note that there has been a significant rise in <strong>predatory journals</strong> — journals that emulate open journals in charging a fee for publication, and providing free access to the published results, but which are often for-profit companies, and either do not require peer review for publishing, or have very low standards for peer review. Scientists concerned with the quality of their work, and how it is perceived by their peers, quickly learn to research publishers prior to submitting manuscripts so as to avoid predatory publishers.</p>
<p>Open journals differ in their approach to the question of impact factor. Some journals explicitly guarantee publication of any research study that is deemed to be scientifically and ethically sound, regardless of its perceived impact. Examples of this include <a class="reference external" href="https://journals.plos.org/plosone/s/journal-information#loc-criteria-for-publication"><em>PLoS ONE</em></a> and <a class="reference external" href="https://www.frontiersin.org/about/review-system#AcceptanceCriteria"><em>Frontiers</em></a>. Individual papers (and the journals, in aggregate), still report impact factors, but anticipated impact is not a criterion for publication. At the same time, some open publishers see the value in curation and have journals that are more selective in the papers they accept. For instance, while PLoS ONE accepts papers based solely on scientific merit, the same organization publishes <em>PLoS Biology</em> and <em>PLoS Medicine</em>, which consider <a class="reference external" href="https://journals.plos.org/plosbiology/s/what-we-publish">“works of exceptional importance”</a>.</p>
<p>Another facet of open publishing is <strong>preprint archives</strong>. These are freely-accessible, web-based repositories on which scientists can post copies of their manuscripts prior to peer review. This has advantages and disadvantages. One advantage is that the world is provided with earlier access to scientific results — although with the caveat that in the absence of peer review, these findings could be incorrect, misleading, or even fraudulent. Good scientists, however, have reputations to build and uphold, and so are usually disincentivized publicly posting incorrect or fraudulent material. Another advantage is that, even if the scientists ultimately choose to publish their work in a “closed” journal, the preprint is still publicly available. Many journals allow the final, accepted version of a paper to be published in an open archive, with the caveat that this must not be the version formatted by the journal. One of the earliest and most prominent of these is <a class="reference external" href="https://arxiv.org/">arXiv</a>, however there are now many others. As well, many universities have their own repositories, providing their faculty and students with an in-house platform for archiving. For example, Dalhousie University’s libraries offer <a class="reference external" href="https://dalspace.library.dal.ca">DalSpace</a>.</p>
</div>
<div class="section" id="preregistration">
<h2>Preregistration<a class="headerlink" href="#preregistration" title="Permalink to this headline">¶</a></h2>
<p>A related concept is <strong>preregistration</strong>. This is a practice in which scientists publish their methods prior to conducting the experiment. Preregistration can either be “embargoed” or not. <strong>Embargoed</strong> means that the researchers register their planned methods with some independent agency, who agrees to keep the methods private until the research is ready to publish the results. This helps researchers protect from being “scooped”, having others copy their methods and beat them to publication. For example, the <a class="reference external" href="https://osf.org">Open Science Framework (OSF)</a> is a not-for-profit organization that provides a web platform on which researchers can pre-register methods and choose to have them embargoed. The web site tracks the dates when information was entered as preregistration, and any edits that were made. Thus the researcher is in control of what information is provided, and when, but the entire history of changes provides others with documentation as to whether the end results, and methods used to obtain them, are free from HARKing and p-hacking.</p>
<p>The other option is to publish the methods as a “preregistered report”. Some journals now allow, and even encourage this. Indeed, some journals will guarantee publication of the results of any study whose preregistered methods that journal publishes. Both the preregistered methods, and the results, still need to get through peer review, but this provides a high level of transparency around methods, preventing HARKing and p-hacking, and ensures that publication is not biased by the perceived interest or novelty of the results. Preregistration is also required in some areas of research, such as clinical trials testing the efficacy of a new drug, device, or other intervention.</p>
</div>
<div class="section" id="open-methods-and-data">
<h2>Open Methods and Data<a class="headerlink" href="#open-methods-and-data" title="Permalink to this headline">¶</a></h2>
<p>You have likely been taught that the Methods section of a published journal article should describe the methods in sufficient detail that they can be replicated by anyone reading them. In practice, this is rarely entirely true. Methods sections gloss over many details, for reasons such as assumptions about what the reader understands, or saving space. Many journals set limits on the length of articles, and usually authors are more concerned with presenting and discussing results and theory, than with the methods. As well, an overly-detailed Methods section can be tedious to read. Indeed, as you will discover in this course, data analysis often involves extremely long and complex computer code, and the “devil is in the details”. Publishing thousands of lines of Python code in a journal article is not feasible.</p>
<p>At the same time, a lack of transparency in methods has contributed to the replicability crisis, and arguably restricted scientific advances in cases where researchers cannot access enough of the methods to perform a replication, or even determine what factors may have contributed to a result. For instance, many studies in psychology and neuroscience rely on presenting stimuli to participants. It is relatively rare for researchers to publish their entire stimulus set (as opposed to one or two examples), and it is not uncommon for old stimuli to simply be lost, so that even if a researcher wanted to share them with a colleague later, they cannot. Likewise, since data analysis code is complex, there is always the possibility that it contains errors — but if the actual code used to obtain a result is not available, no one can audit it to catch those errors.</p>
<p>For these reasons, it is increasingly common for researchers to provide greater transparency in their practice by making all stimuli, analysis code, and data publicly available. The OSF platform mentioned above is one that supports this practice. In addition to pre-registering methods, researchers can post all of their stimuli, analysis code, and data. The OSF platform can also serve as a preprint archive, hosting a copy of the resulting manuscript as well. As with methods, any material placed on OSF can be embargoed, so that researchers have control of when it becomes available to the public.</p>
<p>Increasingly, journals are making the decision to require that researchers make at least their data, if not their analysis code and stimuli, publicly available as a criterion for publication. Beyond transparency and accountability, these open practices can benefit the scientific community in other ways. For example, a given data set may contain interesting information that were not the focus of the analyses performed by the researchers that collected the data. By making data publicly available, other researchers can access it and perform different analyses, or meta-analyses (analyses of results over multiple studies). Given that most research is funded by taxpayer money, this also increases the value of the public investment in science. Another benefit is education. By examining the analysis code from a published study, other researchers may learn new ways of doing things, which benefit their own research. As well, open datasets can be used as sample data in educational settings (such as this course), which allows students to practice data analysis, and has the added benefit in many cases of knowing whether their results are correct, by comparison to the published results.</p>
</div>
</div>


              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="software.html" title="previous page">Free Software</a>
    <a class='right-next' id="next-link" href="oer.html" title="next page">Open Educational Resources</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2020. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 Unported License.<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.0.3.<br/>
    </p>
  </div>
</footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>