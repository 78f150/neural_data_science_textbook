{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Multiple Data Files Using Copilot\n",
    "\n",
    "In this lesson we will revisit some of the material covered at the end of the introductory chapter on Python. Specifically, reading multiple data files, manipulating the data using pandas, and deriving some basic information from the data.\n",
    "\n",
    "The instructions are very high-level, because we want you to work on writing Copilot prompts yourself, based on high-level instructions rather than us giving you step-by step guidance. The version of this lesson in the online textbook, however, shows the worked example with prompts and Copilot-generated code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Your task is to perform the operations below, using Python code generated by a combination of your own writing, and GitHub Copilot's suggestions. For your own learning, you are best off to start each step by trying to write the code yourself. If it works, great – you've learned something in Python! Congratulations. But, tTe tasks below intentionally ask you to do things that we haven't taught in this course yet. That's because we want you to learn how to use Copilot to help you out when you don't know how to do something.\n",
    "\n",
    "If you try writing your own code and it doesn't work, then you can try again, or use Copilot to help you out. Copilot is meant to speed up your coding, so in practice it's preferable to use it to help you out when you get stuck, than spend too much time trying to figure out how to do something yourself. On the other hand, once you have the solution from Copilot, it's important to try to understand what it is doing. You may have to look up some of the functions it's using, if you're not familiar with them. \n",
    "\n",
    "If you're ignorant of what the code you generated is doing, you risk errors. Indeed, in the tutorial below we will see an example of Copilot's generating erroneous code. Even if the code works as desired, it is important for you to be able to explain correctly what you did. Also, look for ways to check that the results you get are correct (using code). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The Data and What to Do With It\n",
    "\n",
    "There are three data files in the `data` directory: `s1.csv`, `s2.csv`, and `s3.csv`. Each file contains the reaction times (RTs) from 10 trials of a relatively simple task in which participants had to indicate which direction a briefly-presented arrow was pointing. The RTs are in seconds (s). Each file contains the RTs from a different participant. In each file there are three columns. You can determine what the columns are by looking at the first row (header) of each file.\n",
    "\n",
    "Generate code that reads in the three files and combines them in a pandas DataFrame. Include error-checking  code that determines you loaded in the correct number of trials (30) and the correct number of columns (3). If the number of trials or columns is incorrect, print an error message.\n",
    "\n",
    "Save the dataframe to a file called `all_data.csv`, in the `data` folder.\n",
    "\n",
    "Once you have the DataFrame, calculate:\n",
    "- the mean RT for each participant\n",
    "- the mean RT across all participants\n",
    "- the 95% confidence intervals (CIs) for the mean RT for each participant\n",
    "- the 95% CIs for the mean RT across all participants\n",
    "\n",
    "Finally, print a table that includes all of the results: with columns for mean RT, lower 95% CI, and upper 95% CI, and with one row for each participant and the bottom row showing the mean and CIs across participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Approach This Exercise\n",
    "\n",
    "A critical skill to develop in coding is **problem decomposition**, or **programmatic thinking** — in other words, breaking down a task into smaller and smaller components, so that you can write the code to perform each step in the logical sequence. The instructions above are written in a sequential way, so you should be able to identify each individual step that  you need to take.\n",
    "\n",
    "It is good practice, when working in Jupyter notebooks, to write the code for each step in a separate cell. This makes it easier to test each step, and to go back and change things if you need to. It also makes it easier to see what you've done, and understand what the code is doing. As well, it's sometime helpful to put Markdown cells in between code cells, to provide longer explanations than might be appropriate for a comment in the code itself. You can also make notes about things you might want to change later, or interpretations of the output of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Copilot Assistant\n",
    "\n",
    "As noted above, we encourage you to try writing the code for each step of the instructions above. Coding is a *procedural* skill that you only learn by doing — and the more you learn the better you will be at solving the bugs that Copilot-generated code will inevitably have. But, if you get stuck, or encounter an instruction that you haven't yet learned how to perform in Python, then by all means use Copilot. Just like writing your own code, it is best if you use a separate notebook cell for each Copilot prompt. That way, if you need to edit the code, you can do so without having to re-run all of the code that Copilot generated. It's also easier to debug because you can see the output of each cell and now exactly which line generated the error.\n",
    "\n",
    "As you will see, it takes some trial and error – and critical thinking – to generate prompts that produce the code you want. But, it's a good way to learn how to use Copilot effectively. And if you do it right, you will learn a lot about Python along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Copilot does not generate the same code every time you type the same prompt. This is the nature of the type of generative AI that Copilot uses. So, when working through this lesson, you may get different outputs than what is shown in the lesson. Try to use the processes explained here for interpreting error messages and Copilot's descriptions and suggested code. It's much more important to get familiar with engaging in the *process* of debugging, through the thought processes explained here, than to worry about getting different suggestions from Copilot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using Copilot to Perform the Tasks Above\n",
    "\n",
    "The cells below all reflect prompts written on the basis of the instructions above, and code generated entirely by Copilot. While your experience may be different, in writing this lesson we only had to type the first 2-line prompt, and Copilot generated not only the code but the other prompts/comments. \n",
    "\n",
    "However, you have to get used to the flow of working with Copilot. After typing the first two-line prompt below, if you only hit `Enter` once, you probably won't get any code generated. Copilot encourages a good coding style, and so it will only generate code if you hit `Enter` twice, so that there's an empty line between your prompt and the code. And in fact, after I typed the first two lines of the prompt and hit `Enter`, Copilot added a third suggested line to the prompt after I hit `Enter` the first time, which was to `write the dataframe to a csv file`. This is pretty cool, since that was also part of the instructions above! Is Copilot a mind-reader? No, it's just a very good predictor of what you might want to do next, based on the prompt you've written. It's a pretty common task in data science to read in a set of individual data files, concatenate them into a single DataFrame, and then save that DataFrame to a file for later use. So, Copilot is just doing what it's trained to do, which is to predict what you might want to do next, based on the prompt you've written.\n",
    "\n",
    "Anyway, after I accepted the third line of the prompt below, I had to hit `Enter` twice to get the first line of code (`import pandas as pd`) generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/s2.csv', 'data/s3.csv', 'data/s1.csv']\n"
     ]
    }
   ],
   "source": [
    "# read in three files from the data folder, whose names start with \"s\" and end in \"csv\"\n",
    "# concatenate them into one dataframe\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# use glob to get all the files that start with \"s\" and end with \"csv\"\n",
    "# glob returns a list of file names\n",
    "filenames = glob.glob(\"data/s*.csv\")\n",
    "print(filenames)\n",
    "\n",
    "# read in the files and concatenate them into one dataframe\n",
    "dataframes = []\n",
    "for f in filenames:\n",
    "    dataframes.append(pd.read_csv(f))\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "df\n",
    "\n",
    "# write the dataframe to a csv file\n",
    "df.to_csv(\"data/concatenated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above appears to have worked fine. Next we'll to some checks to confirm that, as described in the instructions. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Learning Point</b><p>\n",
    "The above appears to have worked fine. However, the above code uses a <code>for</code> loop to iterate over the files and read them in. There is a way we could do this in one line of code, using <em>list comprehension</em>:</p>\n",
    "\n",
    "```python\n",
    "df = pd.concat([pd.read_csv(f) for f in files])\n",
    "```\n",
    "\n",
    "This demonstrates that Copilot can generate working code, but not always the most efficient code.\n",
    "</div>\n",
    "\n",
    "For the moment, we'll stick with the code Copilot generated, and check that it is accurate. We'll start by checking the number of columns in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participantID</th>\n",
       "      <th>trial</th>\n",
       "      <th>RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.433094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.392526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.396831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.417988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.371810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participantID  trial        RT\n",
       "0            s2      1  0.433094\n",
       "1            s2      2  0.392526\n",
       "2            s2      3  0.396831\n",
       "3            s2      4  0.417988\n",
       "4            s2      5  0.371810"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the DataFrame has 3 columns, as expected. Next, we'll check that we get the expected number of rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the dataframe has 30 rows and 3 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also were instructed to raise an error message if the number of rows or columns is incorrect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise an error if the dataframe does not have 30 rows and 3 columns\n",
    "assert df.shape == (30, 3), \"The dataframe does not have 30 rows and 3 columns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Test\n",
    "This generates no output. This is a Good Thing, because it means that the number of rows and columns is correct. However, it's a bit dangerous to assume no news is good news, because no news could also mean your code is not working. So, we should test that the error message is generated if we change the expected number of rows or columns.\n",
    "\n",
    "We don't want to actually remove any rows from the DataFrame, but we can use slicing to create a view of the DataFrame that has fewer rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The dataframe does not have 30 rows and 3 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#Y164sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create a slice of df that contains 29 rows, \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#Y164sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# then raise an error if the dataframe does not have 30 rows and 3 columns\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#Y164sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_slice \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m:\u001b[39m29\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#Y164sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m df_slice\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39m30\u001b[39m, \u001b[39m3\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mThe dataframe does not have 30 rows and 3 columns\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The dataframe does not have 30 rows and 3 columns"
     ]
    }
   ],
   "source": [
    "# create a slice of df that contains 29 rows, \n",
    "# then raise an error if the dataframe does not have 30 rows and 3 columns\n",
    "df_slice = df.iloc[0:29]\n",
    "assert df_slice.shape == (30, 3), \"The dataframe does not have 30 rows and 3 columns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get an `AssertionError`, which is a Good Thing because it confirms that our error-checking code is working. We can do the same thing to test the error-checking code for the number of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The dataframe does not have 30 rows and 3 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#Y200sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create a slice of df that contains 2 columns, \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#Y200sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# then raise an error if the dataframe does not have 30 rows and 3 columns\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#Y200sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_slice \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#Y200sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m df_slice\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39m30\u001b[39m, \u001b[39m3\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mThe dataframe does not have 30 rows and 3 columns\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The dataframe does not have 30 rows and 3 columns"
     ]
    }
   ],
   "source": [
    "# create a slice of df that contains 2 columns, \n",
    "# then raise an error if the dataframe does not have 30 rows and 3 columns\n",
    "df_slice = df.iloc[:, 0:2]\n",
    "assert df_slice.shape == (30, 3), \"The dataframe does not have 30 rows and 3 columns\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yes, our code will throw errors if the number of rows or columns is incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Mean RT for Each Participant... and Our First Bug\n",
    "Our next instruction is to calculate the mean RT for each participant. Let's prompt Copilot to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'subject'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# calculate mean rt for each subject\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39msubject\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   8250\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8252\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   8253\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   8254\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[1;32m   8255\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   8256\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   8257\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[1;32m   8258\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   8259\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[1;32m   8260\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m   8261\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[1;32m   8262\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropna \u001b[39m=\u001b[39m dropna\n\u001b[1;32m    930\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[1;32m    932\u001b[0m         obj,\n\u001b[1;32m    933\u001b[0m         keys,\n\u001b[1;32m    934\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    935\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    936\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    937\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m    938\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[1;32m    939\u001b[0m     )\n\u001b[1;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[1;32m    942\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m    983\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 985\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    986\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subject'"
     ]
    }
   ],
   "source": [
    "# calculate mean rt for each subject\n",
    "df.groupby('subject').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Debugging Copilot-Generated Code\n",
    "\n",
    "Typically, when you get a long, scary error message like the one above, you can ignore a lof of what is in the middle. The most important parts are the last line, which tells you what the error is, and the first lines, which usually indicate what line in the code you tried to run caused the error. What's in between is the *stack trace*, which is a list of all the functions that were called in the process of trying to run the code. But most of the time, the error is a result of the code you wrote (the first lines), not the code in the underlying Python functions that your code called (the middle lines).\n",
    "\n",
    "In this case, we see a `KeyError: 'subject'` at the bottom of the error message. Recall that Python dictionaries are sets of **key**-value pairs. The keys are the names of the columns, and the values are the data in those columns. \n",
    "\n",
    "You can think of a pandas DataFrame as a dictionary in which the column names are keys, and the values in that column are its values. This is a common way that pandas functions refer to column names and their values. So the error message above indicates that the code tried to access a column called `subject` in the DataFrame, but that column doesn't exist. We can see at the top of the error message that the code that caused the error is: `---> 2 df.groupby('subject').mean()`. This means it's line 2 of the cell above that caused the error. \n",
    "\n",
    "As a novice coder, however, you might not know or remember that pandas DataFrames represent column names and data as dictionary key-value pairs. If you don't understand what an error means, you can use Copilot to help you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Copilot to Explain Errors and Suggest Fixes\n",
    "\n",
    "Somewhat counterintuitively, even though Copilot can generate code with bugs, it can also provide feedback on errors in the code it generated. \n",
    "\n",
    "Copilot has a few features that can help you diagnose and fix your code. If you right-click in a code cell, you will see `Copilot` within the menu that pops up. Under that heading there are a number of options, including `Explain This` and `Fix This`. Some work better than others, but we will look at each of them in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Copilot `Explain This`\n",
    "\n",
    "A great place to start in diagnosing errors is `Explain This`. Intuitively, you might go straight for `Fix This`. However, it's probably instructive to see what Copilot thinks your code is doing, and if that is what *you* think your code is doing. Copilot's explanation might give you insight into the error. Below is the output from selecting `Explain This` for the line of code that caused the error above:\n",
    "\n",
    "> This code is designed to calculate the mean reaction time (RT) for each subject in a dataframe. The code uses the groupby() method to group the data by the 'subject' column and then calculates the mean for each group using the mean() method.\n",
    ">\n",
    ">The resulting dataframe contains the mean RT for each subject. The index of the dataframe is the unique values in the 'subject' column, and the columns are the other numerical columns in the original dataframe.\n",
    ">\n",
    "> This code is useful when working with data that has multiple subjects and you want to calculate summary statistics for each subject. By using the groupby() method, you can easily calculate summary statistics for each group without having to manually subset the data.\n",
    ">\n",
    "> One possible way to improve this code would be to specify the column to calculate the mean for, rather than calculating the mean for all numerical columns. This would make the code more specific and reduce the risk of errors. Additionally, the code could be modified to calculate other summary statistics, such as the standard deviation or median, by using the appropriate method instead of the mean() method.\n",
    "\n",
    "First off, this shows how Copilot can be a great learning tool. It's giving us a clear explanation of what the code is doing, and teaching us a bit about how the `.groupby()` method works. It also suggests some improvements to the code, which are actually good suggestions.\n",
    "\n",
    "None of this explanation tells us directly why the code generates an error, but as a novice coder this probably provided you with greater understanding of what the code is actually doing. This will be helpful as we try to figure out what is going wrong. \n",
    "\n",
    "The first paragraph of Copilot's explanation *does* provide a subtle clue to us about the error, though. It notes that the code is grouping the data by the `subject` column. The error message also referenced `subject`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copilot `Fix This`\n",
    "\n",
    "If you select `Fix This` from the Copilot menu, you will see what's called a **diff**, as shown below. A diff is a way of showing the difference between two pieces of code. The code on the left (with a pink background) is the code you wrote, and the code on the right (with a green background) is the code that Copilot suggests. In this case, the suggested \"fix\" would not fix the problem. \n",
    "\n",
    "![](images/copilot_fix_this.png)\n",
    "\n",
    "\n",
    "The suggested code is doing two things differently from the original code. Firstly, it's suggesting that you assign the output of the command to a variable, and then print the variable. Assigning output to a variable and then printing it is in general good coding style, because we may want to use that variable again later in our code. But that won't fix the error. \n",
    "\n",
    "The other difference – which is subtle to a novice coder – is that the new code is referencing the column `Subject` rather than `subject`. Recall that Python is **case-sensitive**, meaning that `Subject` and `subject` are not the same thing. Copilot seems to be guessing that the error might be cause by a common mistake, which is to refer to a variable using the wrong case. \n",
    "\n",
    "In this particular situation, however, the error is not caused by a case-sensitivity error. The error is caused by the fact that the column `subject` doesn't exist in the DataFrame.\n",
    "\n",
    "So, you can click the `Discard` button to reject Copilot's suggestion, and the `Escape` key to exit the diff view.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](images/chat_icon.png)  Copilot `Chat` \n",
    "\n",
    " Another, really useful option is the `Chat` option, which you can option by clicking the `Chat` icon (shown above) in the Activity Bar (left side of VS Code). This allows you to have a conversation with Copilot about what you want to do. If you've used [Chat-GPT](https://chat.openai.com/) before, this will be familiar to you as it's essentially the same type of natural-language interface.\n",
    "\n",
    "There are different approaches to using Copilot Chat to help fix an error. One is to click in the cell that generates the error, and then click in the Copilot Chat input box, and typing: \n",
    "\n",
    "```\n",
    "the selected cell generates an error. how can i fix this error?\n",
    "``` \n",
    "\n",
    "In response to this prompt for the present error, I got the following response:\n",
    "\n",
    ">If you are getting an error with the selected cell, there could be a few reasons why. Here are some possible solutions:\n",
    ">\n",
    ">1. Check that the 'subject' column is present in the dataframe. You can do this by printing out the dataframe using the `print()` function and checking that the 'subject' column is present.\n",
    ">\n",
    ">2. Check that the 'subject' column is spelled correctly. If the column is misspelled, you will get a \"KeyError\" when trying to group by that column.\n",
    ">\n",
    ">3. Check that the 'subject' column is of the correct data type. If the 'subject' column is not a string or categorical data type, you may need to convert it using the `astype()` method.\n",
    ">\n",
    ">4. Check that the dataframe contains numerical columns. If the dataframe only contains non-numerical columns, you will get a \"DataError\" when trying to calculate the mean.\n",
    ">\n",
    ">5. Check that the dataframe does not contain missing values. If the dataframe contains missing values, you may need to remove them using the `dropna()` method before calculating the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Error with Copilot's Help\n",
    "\n",
    "These are actually really good responses, and they are provided in a logical order to go through in debugging. We could work through them in order, but — spoiler alert — the first suggestion actually leads to a solution. \n",
    "\n",
    "### Check that the 'subject' Column is Present in the DataFrame\n",
    "The first thing to do is to check that the column exists, and is spelled correctly. So let's start with that. Do you remember how to check the names of the columns in a pandas DataFrame? If not, you can write a Copilot prompt to help you out. In the cell below, write a prompt that will print the names of the columns in the DataFrame. Then run the cell, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# print the column names of the dataframe\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# print the column names of the dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to look at the first few rows of the DataFrame, which includes the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participantID</th>\n",
       "      <th>trial</th>\n",
       "      <th>RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.433094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.392526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.396831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.417988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.371810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participantID  trial        RT\n",
       "0            s2      1  0.433094\n",
       "1            s2      2  0.392526\n",
       "2            s2      3  0.396831\n",
       "3            s2      4  0.417988\n",
       "4            s2      5  0.371810"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third option, when using Jupyter notebooks with VS Code, is to click on the `Variables` button in the toolbar at the top of the notebook window. This will pop up a variable viewer in sub-window below your notebook. You can click on the variable names to see their values. For DataFrames, it actually shows a list of the columns in the window, and you can double-click on the variable name to see the contents of the DataFrame in another window, the **Data Viewer**. This view is similar to a spreadsheet. In fact, you can directly edit values in the Data Viewer. *You should never directly edit values like this*, however. Any steps you do manually are not documented in your code, and are not reproducible.  \n",
    "\n",
    "The screenshot below shows the variables and Data Viewer for the current context.\n",
    "\n",
    "![](images/inspectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution\n",
    "\n",
    "Using any of the three approaches above, when we look at the column names, we see that they are `participantID`, `trial`, and `RT`. The code that generated the error was trying to access a column called `subject`, which doesn't exist. It should be `participantID`. So we need to change the code to access the correct column name. Or, engineer our prompt to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>RT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participantID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s1</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.389548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.444785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.446009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               trial        RT\n",
       "participantID                 \n",
       "s1               5.5  0.389548\n",
       "s2               5.5  0.444785\n",
       "s3               5.5  0.446009"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean rt for each participantID\n",
    "df.groupby('participantID').mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, however the code is providing means for both columns in the DataFrame (`trial` and `RT`), not just for `RT` (sometimes the same prompt actually does select only `RT` but we'll explore when it doesn't). We can add to our prompt to tell it not to include `trial` in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean rt for each subject. Do not show the mean for trial \n",
    "df.groupby('participantID').mean().drop('trial', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above generated code does what we want. However, from the perspectives of coding style and efficiency, it's not optimal. Python executes this **chained command** from left to right. So, it first computes the mean for each column in the DataFrame, and then drops the column `trial`.\n",
    "\n",
    "It seems unnecessary to compute the mean for `trial` and then drop it. This isn't really Copilot's fault — we did explicitly tell it not to show the mean for trial, but it's not smart enough to know that we don't want to compute it in the first place; it seems to have interpreted our prompt as a literal sequence of commands.\n",
    "\n",
    "We can modify the prompt in a way that generates more efficient code, by being specific about the column that we do want, rather than what we don't want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "participantID\n",
       "s1    0.389548\n",
       "s2    0.444785\n",
       "s3    0.446009\n",
       "Name: RT, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean for each participantID using the RT column\n",
    "df.groupby('participantID')['RT'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By way of showing how sensitive Copilot is to the structure of your prompt, a slightly different (and arguably more logical) phrasing of the prompt above generates the less-efficient code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participantID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s1</th>\n",
       "      <td>0.389548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <td>0.444785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>0.446009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     RT\n",
       "participantID          \n",
       "s1             0.389548\n",
       "s2             0.444785\n",
       "s3             0.446009"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean RT for each participantID\n",
    "df.groupby('participantID').mean().drop('trial', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "One thing you may notice is that the result of the last command above is nicely-formatted when it is displayed, whereas the one before it is in a more \"raw\" format. This is not really important here, but it's worth understanding why the difference occurs. When you call a pandas DataFrame it prints in a nicely formatted output. However, when you call a pandas Series (which is a single column), it prints in a more detailed but less \"pretty\" way. \n",
    "\n",
    "In the output immediately above, the code created a DataFrame with two columns (`trial` and `RT`) and then dropped the `trial` column, but as such it remained a DataFrame and so was nicely formatted.\n",
    "\n",
    "In contrast, the output of using the `mean()` method on a single column (`RT`) in the cell above that is a Series.  \n",
    "<p><p>\n",
    "We'll worry about the formatting later, but it's good to understand why it happens because the distinction between DataFrames and Series often causes confusion and errors if it's not understood.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Continue Coding\n",
    "\n",
    "Having fixed that error, we can now move on with the steps in the instructions. The next step was to calculate the mean RT across subjects. Do that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb Cell 42\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# calculate the mean rt across all subjects\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rt'"
     ]
    }
   ],
   "source": [
    "# calculate the mean rt across all subjects\n",
    "df['rt'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, when I tried this, I got another `key Error`, this time for `rt`. This is because the code is trying to access a column called `rt`, but the column name is actually `RT`. Again, let's start by trying to fix this ourselves, and then use Copilot if we get stuck. \n",
    "\n",
    "The code above selects the column in `df` called `rt` and computes its mean. However, if you look back to the listing of column names that we generated earlier, you'll see that the column name is actually `RT`. Remember that Python is **case sensitive**, meaning that capitalization matters: `RT` and `rt` are not the same. So we need to change the code to access the correct column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4267806816333334"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['RT'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fixed that error, let's move on to the next step, which is to calculate the 95% confidence intervals for the mean RT for each participant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'subject'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb Cell 46\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# calculate 95% confidence intervals for each subject\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aaron/3505/NESC_3505_textbook/3b-ai_assisted/multi_data_files.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39msubject\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39msem() \u001b[39m*\u001b[39m \u001b[39m1.96\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   8250\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8252\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   8253\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   8254\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[1;32m   8255\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   8256\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   8257\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[1;32m   8258\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   8259\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[1;32m   8260\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m   8261\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[1;32m   8262\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropna \u001b[39m=\u001b[39m dropna\n\u001b[1;32m    930\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[1;32m    932\u001b[0m         obj,\n\u001b[1;32m    933\u001b[0m         keys,\n\u001b[1;32m    934\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    935\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    936\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    937\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m    938\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[1;32m    939\u001b[0m     )\n\u001b[1;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[1;32m    942\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m    983\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 985\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    986\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subject'"
     ]
    }
   ],
   "source": [
    "# calculate 95% confidence intervals for each subject\n",
    "df.groupby('subject').mean().apply(lambda x: x.sem() * 1.96, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we get `Key Error: 'subject'`, which is the same as the first error above. So again, Copilot is labelling the column incorrectly. We know that we can fix this by replacing `subject` with `participantID`, but it's a bit annoying that Copilot keeps making this error. On the other hand, many errors are actually human errors. We are prompting Copilot to do something for each  \"subject\", which then requires Copilot to know that by \"subject\" we really mean the column called `participantID`. Maybe we could engineer our prompt to be better instead, by referring to the column name directly. Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "participantID\n",
       "s1    5.008243\n",
       "s2    4.954111\n",
       "s3    4.952911\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate 95% confidence intervals for each participantID\n",
    "df.groupby('participantID').mean().apply(lambda x: x.sem() * 1.96, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pink Elephants\n",
    "\n",
    "This is a frightening example of a Copilot **hallucination**. The above code generates a table of numbers, one for each participantID.  You might think this is a plausible result. However, there are a couple of problems. Firstly, confidence intervals (CIs) are typically reported as a pair of values. The 95% CIs reflect the *range* within which it is 95% likely that the true mean lies, so we need an upper *and* lower limits of the CI. These are typically the same amount plus or minus the mean (e.g., if the mean is 2, the 95% CIs might be 1.5 and 2.5). So given a conceptual understanding of what CIs are, we recognize that we need to modify the code to generate the upper and lower values of the CIs. \n",
    "\n",
    "Secondly, if you look back at the mean RT for each participant, they were in the range of .339 – .449. Again, confidence intervals reflect the range of possible true values for the mean, given the sample data.  So for our confidence intervals to have values around 5 s seems implausible, given the mean RTs are < 500 ms. We would expect the CIs to be perhaps 100-200 ms. So it seems like Copilot is calculating the CIs incorrectly.\n",
    "\n",
    "This highlights the critical importance of understanding not only what your code is doing, but what you are asking it to do. You should always check the output of your code to make sure it makes sense, and also make sure you understand what you are supposed to be doing with the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copilot Chat to the Rescue\n",
    "\n",
    "Since Copilot Chat was helpful before, let's try using it to help us figure out what is going on. If we click on the `Chat` icon in the Activity Bar, we can type in a question to Copilot. In this case, I asked Copilot the following:\n",
    "\n",
    "> the highlighted code is supposed to compute the 95% confidence intervals of mean RT. However, the output values are incorrect. Can you please suggest code that correctly calculates the 95% confidence intervals, and provides both the upper and lower CIs in a table\n",
    "\n",
    "Copilot provided the answer below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> To correctly calculate the 95% confidence intervals for the mean RT and provide both the upper and lower CIs in a table, you can modify the existing code as follows:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# calculate mean and standard error of the mean for each participant\n",
    "mean_rt = df.groupby('participantID')['RT'].mean()\n",
    "sem_rt = df.groupby('participantID')['RT'].sem()\n",
    "\n",
    "# calculate 95% confidence intervals for the mean RT\n",
    "ci_rt = sem_rt * 1.96\n",
    "\n",
    "# calculate upper and lower CIs\n",
    "upper_ci = mean_rt + ci_rt\n",
    "lower_ci = mean_rt - ci_rt\n",
    "\n",
    "# create a table of the upper and lower CIs\n",
    "ci_table = pd.DataFrame({'Upper CI': upper_ci, 'Lower CI': lower_ci})\n",
    "\n",
    "# print the table of CIs\n",
    "print(ci_table)\n",
    "```\n",
    "\n",
    ">> In this code, the `groupby()` method is used to group the data by the 'participantID' column, and the `mean()` and `sem()` methods are used to calculate the mean and standard error of the mean for the 'RT' column for each group. The resulting series contains the mean and SEM for each unique value in the 'participantID' column.\n",
    ">> \n",
    ">> The `ci_rt` variable calculates the 95% confidence intervals for the mean RT using the formula `SEM * 1.96`, where `SEM` is the standard error of the mean.\n",
    ">> \n",
    ">> The `upper_ci` and `lower_ci` variables calculate the upper and lower CIs by adding and subtracting the `ci_rt` value from the `mean_rt` value, respectively.\n",
    ">> \n",
    ">> The `ci_table` variable creates a table of the upper and lower CIs using the `pd.DataFrame()` function.\n",
    ">> \n",
    ">> Finally, the `print()` function is used to print the table of CIs.\n",
    ">> \n",
    ">> This code should give you the desired output of a table containing the upper and lower CIs for the mean RT for each participant.\n",
    "\n",
    "If you move your cursor over the code that Copilot generated, you will see a `Copy` button appear. Clicking this will copy the code to your clipboard, so you can paste it into your notebook. We'll run this code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Upper CI  Lower CI\n",
      "participantID                    \n",
      "s1             0.430626  0.348470\n",
      "s2             0.496197  0.393373\n",
      "s3             0.515907  0.376111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate mean and standard error of the mean for each participant\n",
    "mean_rt = df.groupby('participantID')['RT'].mean()\n",
    "sem_rt = df.groupby('participantID')['RT'].sem()\n",
    "\n",
    "# calculate 95% confidence intervals for the mean RT\n",
    "ci_rt = sem_rt * 1.96\n",
    "\n",
    "# calculate upper and lower CIs\n",
    "upper_ci = mean_rt + ci_rt\n",
    "lower_ci = mean_rt - ci_rt\n",
    "\n",
    "# create a table of the upper and lower CIs\n",
    "ci_table = pd.DataFrame({'Upper CI': upper_ci, 'Lower CI': lower_ci})\n",
    "\n",
    "# print the table of CIs\n",
    "print(ci_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this is great progress. We now have our output in a table with both upper and lower CIs, and we can see that the CIs are in the range we would expect. It wouldn't hurt to cross-check the formula that Copilot is using against a textbook or online resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Check Copilot's Code with Other Sources\n",
    "\n",
    "There's an old saying, \"once bitten, twice shy\". We've already seen that Copilot can generate code that is incorrect. So, it's a good idea to check the code that Copilot generates against other sources. In this case, we can look up the formula for calculating the 95% CI for the mean, and compare it to the code that Copilot generated.\n",
    "\n",
    "If we do a Web search for \"confidence interval formula\", we will find that the results across all of the top hits indicate that the formula for calculating the 95% CI for the mean is:\n",
    "\n",
    "```python\n",
    "ci_upper = mean + 1.96 * (std / sqrt(n))\n",
    "ci_lower = mean - 1.96 * (std / sqrt(n))\n",
    "```\n",
    "\n",
    "where `mean` is the mean of the data, `std` is the standard deviation of the data, and `n` is the number of observations.\n",
    "\n",
    "However, Copilot's code multiplied 1.96 by `sem_rt`, which is the *standard error* of the mean (if you ddin't know that, you could find out via a Web search or asking Copilot chat). But if you remember (or look up) the formula for standard error, it is the standard deviation of the data divided by the square root of the number of observations. So, Copilot's code is correct, it's just using a different formula than the one we found online. But at least now we can be confident that the code is doing what we want it to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95% CIs across all participants\n",
    "\n",
    "Our next instruction is to compute 95% CIs across all participants. Given our troubles above, we may not have much confidence that it will work the first time. But, let's start by copying the  phrasing that Copilot Chat used when describing its answer to the previous question, but adding in \"across all participantID\" to the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Upper CI  Lower CI\n",
      "participantID                    \n",
      "s1             0.430626  0.348470\n",
      "s2             0.496197  0.393373\n",
      "s3             0.515907  0.376111\n"
     ]
    }
   ],
   "source": [
    "# calculate the 95% confidence intervals for the mean RT across all participantID\n",
    "# and provide both the upper and lower CIs in a table\n",
    "mean_rt = df.groupby('participantID')['RT'].mean()\n",
    "sem_rt = df.groupby('participantID')['RT'].sem()\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = mean_rt + ci_rt\n",
    "lower_ci = mean_rt - ci_rt\n",
    "ci_table = pd.DataFrame({'Upper CI': upper_ci, 'Lower CI': lower_ci})\n",
    "print(ci_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately Copilot doesn't seem to have understood \"across all participantID\", and it gave us CIs for each participant separately. So, let's try again, but this time we'll be more explicit about what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Upper CI  Lower CI\n",
      "RT  0.459111   0.39445\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean RT across all participantID\n",
    "# then compute the 95% confidence intervals for the mean RT across all participantID\n",
    "# and provide both the upper and lower CIs in a table\n",
    "mean_rt = df['RT'].mean()\n",
    "sem_rt = df['RT'].sem()\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = mean_rt + ci_rt\n",
    "lower_ci = mean_rt - ci_rt\n",
    "ci_table = pd.DataFrame({'Upper CI': upper_ci, 'Lower CI': lower_ci}, index=['RT'])\n",
    "print(ci_table) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better — there's only one pair of CIs, for the whole group — and if we examine the code, we see that it's using the same formula as the (correct) code that Copilot generated for the previous step. \n",
    "\n",
    "The prompt above is an example of how **programmatic thinking** is required even when using an AI coding assistant. In order to get the desired result, we had to break down the problem into smaller chunks. In this case, we told it to compute the mean across all participantID, and *then* compute the 95% CIs.\n",
    "\n",
    "Another prompt that also works is below; note that the output is formatted differently, but the values are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.45911120518780957, 0.3944501580788572)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the upper and lower 95% confidence interval limits for RT across participantid\n",
    "df['RT'].mean() + df['RT'].sem() * 1.96, df['RT'].mean() - df['RT'].sem() * 1.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code illustrates a feature of Python that we haven't seen before: using a comma to separate two commands. This is a way of executing two commands in a single line of code. It's not a very common way of writing code, but as you can see in this example, it can be useful: the output is a Python `tuple`, which is an immutable, ordered collection of values. This is a good type to use when you have a pair of values that belong together, and the immutable nature of tuples means that you can't accidentally change one of the values. Furthermore, the ordering means you can access the values by their position in the tuple, which is useful in this case because, using this prompt, Copilot will always put the upper CI first and the lower CI second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Formatting the output nicely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final instruction was, \"Finally, show a table that includes all of the results: with columns for mean RT, lower 95% CI, and upper 95% CI, and with one row for each participant and the bottom row showing the mean and CIs across participants.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Mean RT  Lower CI  Upper CI\n",
      "participantID                             \n",
      "s1                0.39    0.3508    0.4292\n",
      "s2                0.44    0.3812    0.4988\n",
      "s3                0.45    0.3716    0.5284\n"
     ]
    }
   ],
   "source": [
    "# show a table that has columns for mean RT, lower 95% CI, and upper 95% CI, \n",
    "# each rounded to 2 decimal places,\n",
    "# with one row for each participantID, \n",
    "# and the final row for the mean across all participantIDD\n",
    "mean_rt = df.groupby('participantID')['RT'].mean().round(2)\n",
    "sem_rt = df.groupby('participantID')['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = mean_rt + ci_rt\n",
    "lower_ci = mean_rt - ci_rt\n",
    "ci_table = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci})\n",
    "print(ci_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a decent first pass, but the CIs are rounded to 4, not 2, decimal places, and the table is missing the mean across participants. These are two different problems, so let's try to solve them one at a time. We'll start with rounding. Our prompt above was pretty specific in asking Copilot to round each of the stated values to 2 decimal places, so maybe instead of prompt engineering we can ask Chat for help. Let's try this prompt in Copilot Chat: `modify the code in the selected cell so that the CIs are rounded to 2 decimal places`. The result is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To modify the code to round the CIs to 2 decimal places, you can change the `round()` method to round to 2 decimal places instead of the default 4 decimal places. Here's the modified code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "mean_rt = df.groupby('participantID')['RT'].mean().round(2)\n",
    "sem_rt = df.groupby('participantID')['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci})\n",
    "print(ci_table)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this code, the `round()` method is used to round the mean RT and SEM values to 2 decimal places, and the `round()` method is also used to round the upper and lower CI values to 2 decimal places.\n",
    ">\n",
    "> The `upper_ci` and `lower_ci` variables calculate the upper and lower CIs by adding and subtracting the `ci_rt` value from the `mean_rt` value, respectively. The `round()` method is used to round the resulting values to 2 decimal places.\n",
    "> \n",
    "> Finally, the `ci_table` variable creates a table of the mean RT, lower CI, and upper CI values using the `pd.DataFrame()` function, and the `print()` function is used to print the table.\n",
    "> \n",
    "> This code should give you the desired output of a table containing the mean RT, lower CI, and upper CI values rounded to 2 decimal places.\n",
    "\n",
    "And indeed, as you can see below, it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Mean RT  Lower CI  Upper CI\n",
      "participantID                             \n",
      "s1                0.39      0.35      0.43\n",
      "s2                0.44      0.38      0.50\n",
      "s3                0.45      0.37      0.53\n"
     ]
    }
   ],
   "source": [
    "mean_rt = df.groupby('participantID')['RT'].mean().round(2)\n",
    "sem_rt = df.groupby('participantID')['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci})\n",
    "print(ci_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's address the problem of the missing mean across participants. Based on what we learned earlier in the course, this might not be surprising. While we can use `.groupby()` to split the data by participantID, or simply `.mean()` to get the average across participantIDs, there is no way of generating both split and averaged data using the same command. \n",
    "\n",
    "Instead, what we could try doing is create the across-subject data as a separate table, and then merge that table with the by-participant table. In writing the prompt below I also tried a slightly different way of asking for rounding to 2 decimal places, which turns out to be effective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mean RT  Lower CI  Upper CI\n",
      "RT     0.43      0.39      0.47\n"
     ]
    }
   ],
   "source": [
    "# show a table with columns for mean RT, lower 95% CI, and upper 95% CI,\n",
    "# averaged across all participant ID\n",
    "# round all values in the table to 2 decimal places\n",
    "mean_rt = df['RT'].mean().round(2)\n",
    "sem_rt = df['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci}, index=['RT'])\n",
    "print(ci_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to merge the two tables. In the cell below, I only had to type the first one-line prompt, and Copilot generated all the subsequent code and prompts/comments. However, I had to hit `Enter` twice after the first prompt, and after some other lines (where you see blank lines below) to get all the code. I would tell when Copilot had finished because it started regenerating the same prompts and code it had already generated.\n",
    "\n",
    "Unfortunately when we try to run the cell below we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9s/6fvdmxlx00z9sz6ftkrnh8dw0000gn/T/ipykernel_31762/2646225902.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlower_ci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean_rt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mci_rt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mci_table2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Mean RT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmean_rt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Lower CI'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlower_ci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Upper CI'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupper_ci\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# merge the two tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mci_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mci_table2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mambaforge/envs/ncil/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# merge the two tables that are the output of each of the two code cells above\n",
    "\n",
    "# calculate the mean RT for each participantID\n",
    "mean_rt = df.groupby('participantID')['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT for each participantID\n",
    "sem_rt = df.groupby('participantID')['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci})\n",
    "\n",
    "# calculate the mean RT across all participantID\n",
    "mean_rt = df['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT across all participantID\n",
    "sem_rt = df['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table2 = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci}, index=['RT'])\n",
    "\n",
    "# merge the two tables\n",
    "ci_table.append(ci_table2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message is that `'DataFrame' object has no attribute 'append'`. This means that the code is trying to append the across-subject table to the by-participant table, but that the `append()` method doesn't exist for DataFrames. \n",
    "\n",
    "If you do a Web search for \"pandas dataframe append\", you may find this error puzzling, because there are many descriptions of how to use this method, including in the official pandas documentation. However, what's not immediately obvious is that the `.append()` method in pandas was **depricated** in [November, 2021](https://github.com/pandas-dev/pandas/pull/44539). \"Depricated\" in a programming language means that a feature/function/method/etc. was made obsolete — typically replaced by a newer feature, or perhaps because it was already redundant with another way of doing the same thing. Since Copilot is trained on a vast amount of code, this will inevitably include **legacy code** that is no longer used. \n",
    "\n",
    "We'll ask Copilot Chat for help again, this time telling it what the error message is, and asking how to fix it:\n",
    "\n",
    "`the selected cell generates an error: \"'DatFrame' object has no attribute 'append'\". how can i fix this error?`\n",
    "\n",
    "The response is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The error message \"'DataFrame' object has no attribute 'append'\" suggests that the `append()` method is not available for the `DataFrame` object in the code.\n",
    "> \n",
    "> To fix this error, you can use the `concat()` function instead of the `append()` method to concatenate the two dataframes vertically. Here's the modified code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# merge the two tables that are the output of each of the two code cells above\n",
    "\n",
    "# calculate the mean RT for each participantID\n",
    "mean_rt = df.groupby('participantID')['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT for each participantID\n",
    "sem_rt = df.groupby('participantID')['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci})\n",
    "\n",
    "# calculate the mean RT across all participantID\n",
    "mean_rt = df['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT across all participantID\n",
    "sem_rt = df['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table2 = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci}, index=['RT'])\n",
    "\n",
    "# concatenate the two tables vertically\n",
    "ci_table = pd.concat([ci_table, ci_table2])\n",
    "\n",
    "# print the merged table\n",
    "print(ci_table)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this code, the `concat()` function is used to concatenate the two dataframes `ci_table` and `ci_table2` vertically. The resulting dataframe contains the rows of both dataframes stacked on top of each other.\n",
    "> \n",
    "> Finally, the `print()` function is used to print the merged table.\n",
    "> \n",
    "> This code should fix the error and give you the desired output of a merged table containing the mean RT, lower CI, and upper CI values for each participant, as well as the mean RT, lower CI, and upper CI values across all participants.\n",
    "\n",
    "When we try this, it works. The only lingering issue is that the last row is labelled `RT`, which does not tell us the values represent the mean across all participants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mean RT  Lower CI  Upper CI\n",
      "s1     0.39      0.35      0.43\n",
      "s2     0.44      0.38      0.50\n",
      "s3     0.45      0.37      0.53\n",
      "RT     0.43      0.39      0.47\n"
     ]
    }
   ],
   "source": [
    "# merge the two tables that are the output of each of the two code cells above\n",
    "\n",
    "# calculate the mean RT for each participantID\n",
    "mean_rt = df.groupby('participantID')['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT for each participantID\n",
    "sem_rt = df.groupby('participantID')['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci})\n",
    "\n",
    "# calculate the mean RT across all participantID\n",
    "mean_rt = df['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT across all participantID\n",
    "sem_rt = df['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table2 = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci}, index=['RT'])\n",
    "\n",
    "# concatenate the two tables vertically\n",
    "ci_table = pd.concat([ci_table, ci_table2])\n",
    "\n",
    "# print the merged table\n",
    "print(ci_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can as Chat to do this for us: `modify the code in the selected cell so that the last row in the final dataframe is labelled \"Average\"`\n",
    "\n",
    "> To modify the code to label the last row in the final dataframe as \"Average\", you can change the index label of the last row in the `ci_table2` dataframe to \"Average\" before concatenating the two dataframes. Here's the modified code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# merge the two tables that are the output of each of the two code cells above\n",
    "\n",
    "# calculate the mean RT for each participantID\n",
    "mean_rt = df.groupby('participantID')['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT for each participantID\n",
    "sem_rt = df.groupby('participantID')['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci})\n",
    "\n",
    "# calculate the mean RT across all participantID\n",
    "mean_rt = df['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT across all participantID\n",
    "sem_rt = df['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table2 = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci}, index=['Average'])\n",
    "\n",
    "# concatenate the two tables vertically\n",
    "ci_table = pd.concat([ci_table, ci_table2])\n",
    "\n",
    "# print the merged table\n",
    "print(ci_table)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this code, the `index` parameter of the `pd.DataFrame()` function is used to set the index label of the last row in the `ci_table2` dataframe to \"Average\".\n",
    "> \n",
    "> The `concat()` function is then used to concatenate the two dataframes `ci_table` and `ci_table2` vertically. The resulting dataframe contains the rows of both dataframes stacked on top of each other.\n",
    "> \n",
    "> Finally, the `print()` function is used to print the merged table.\n",
    "> \n",
    "> This code should give you the desired output of a merged table containing the mean RT, lower CI, and upper CI values for each participant, as well as the mean RT, lower CI, and upper CI values across all participants, with the last row labelled as \"Average\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Mean RT  Lower CI  Upper CI\n",
      "s1          0.39      0.35      0.43\n",
      "s2          0.44      0.38      0.50\n",
      "s3          0.45      0.37      0.53\n",
      "Average     0.43      0.39      0.47\n"
     ]
    }
   ],
   "source": [
    "# merge the two tables that are the output of each of the two code cells above\n",
    "\n",
    "# calculate the mean RT for each participantID\n",
    "mean_rt = df.groupby('participantID')['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT for each participantID\n",
    "sem_rt = df.groupby('participantID')['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci})\n",
    "\n",
    "# calculate the mean RT across all participantID\n",
    "mean_rt = df['RT'].mean().round(2)\n",
    "\n",
    "# calculate the 95% confidence intervals for the mean RT across all participantID\n",
    "sem_rt = df['RT'].sem().round(2)\n",
    "ci_rt = sem_rt * 1.96\n",
    "upper_ci = (mean_rt + ci_rt).round(2)\n",
    "lower_ci = (mean_rt - ci_rt).round(2)\n",
    "ci_table2 = pd.DataFrame({'Mean RT': mean_rt, 'Lower CI': lower_ci, 'Upper CI': upper_ci}, index=['Average'])\n",
    "\n",
    "# concatenate the two tables vertically\n",
    "ci_table = pd.concat([ci_table, ci_table2])\n",
    "\n",
    "# print the merged table\n",
    "print(ci_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've now completed the task of calculating the mean RT and 95% CIs for each participant, and for all participants. We've also learned a lot about how to use Copilot to help us write code, and how to use Copilot Chat to help us understand errors and fix them.\n",
    "\n",
    "As you've seen, the process of using AI to generate code isn't always smooth, nor are the results always accurate. It's critical to critically assess the outputs you see, and to understand what you are asking Copilot to do — as well as what the code it generates is doing. In this lesson, for the sake of expediency and length we did not provide detailed explanations of what the code was doing every step of the way. However, you've seen that Copilot can be useful at explaining code for you. \n",
    "\n",
    "There were also many points in this lesson where, as an experienced coder, I knew what the problem was and how to fix it. However, I deliberately did not fix the problems myself, because I recognize that as a novice coder you may not know how to fix the problems. Hopefully this lesson has shown you how to use Copilot effectively, but also underscored the value of actually knowing Python and being able to read and debug code yourself.\n",
    "\n",
    "The best approach to learning to use Python for data science is to write code yourself when you can,  because this helps build and reinforce your knowledge (remember, coding is a procedural skill that you only learn by doing — and you lose the knowledge if you don't use it). But, when you encounter a problem you've never solved before, or are getting errors in your Code, Copilot can be a huge help to deepen your knowledge and solve problems. As we've seen, Copilot Chat can be much more useful in this regard than simply writing prompts in code cells of your notebook, because it gives you both code, and detailed explanations of what that code is doing. \n",
    "\n",
    "### Copilot in Later Chapters of this Book\n",
    "\n",
    "Throughout most of the rest of this book, we focus on teaching you to write code yourself, rather than using Copilot. We believe — and feel this lesson has demonstrated — that learning to write code yourself is the best way to learn to code. However, whether you're taking this course for credit and have assignments that Copilot may be useful in helping with, or simply using this book to learn how to apply Python to your own data, we recognize that Copilot can be a useful tool. We hope this chapter has given you a good introduction to how to use Copilot effectively, and that you will continue to use it on your coding journey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
