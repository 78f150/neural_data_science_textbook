# Open Science

[Open science](https://en.wikipedia.org/wiki/Open_science) is a movement to make the practice of scientific research, and dissemination of its results, freely accessible. In the present day, as a student you may find that much of the scientific literature is readily accessible, in the form of journal articles made available through your university's library. However, it is important to realize that access to most of those journals is available only because the university's library pays a subscription fee to the journal publishers. Without a university affiliation, you would have to pay a (usually substantial) fee to access any article.

The open science movement addresses more than just scientific publishing, however. It also covers scientific practice, including openness and transparency in methodology, and free and open data sharing. Below we cover dissemination and methodology in more detail. The following section covers {doc}`oer`, which are also considered part of the open science movement, insofar as education is necessary for good scientific practice.

## Peer Review

As a science student, you have no doubt learned that the "gold standard" of scientific evidence is empirical research published in peer-reviewed journals. Peer review is a hallmark of modern scientific practice and credibility, whereby scientists review each others' work. This is typically done in a "blinded" fashion, whereby the authors of a paper, at least, do not know who the reviewers of the paper are (sometimes reviews are double-blinded, such that reviewers do not see the names or affiliations of the authors). As well, reviewers must declare that they have no conflict of interest in reviewing the work, such as a personal or professional relationship with the authors, or a financial incentive to recommend or prevent publication of the work. Peer reviewers are also not paid to perform this duty, but the vast majority of scientists engage in peer review for journals through a sense of collective duty; firstly, these scientists rely on their peers to review their own work, and secondly, peer review allows scientists to maintain quality control over their discipline. An added bonus is that reviewers get to see the latest scientific developments before others do.

While the principles of the peer review process are sound, they are not without problems. Firstly, the process typically involves an editor for the journal receiving a manuscript, and sending it out for peer review. Many papers are only reviewed by one or two peers, though this may go up to 5 or 6, but rarely more. Thus the people who review a paper may not be truly representative of the views of the wider scientific community. Furthermore, since peer review is unpaid labour, editors must rely on the good will — and availability — of reviewers. Editors also often do not know the people they invite as reviewers well, if at all, and so rely on a general principles of honesty and good faith (e.g., that a reviewer has appropriate expertise, and reviews with a neutral perspective).  This process is thus prone to selection biases, whereby the people who provide reviews may neither be ideally-suited for the job, nor entirely neutral. Indeed, there are numerous cases of papers being rejected by reviewers who clearly were incentivized to do so by a desire to publish similar work first, or even steal ideas and use them as their own.

These issues have led to the development of **open peer review**. In this process, the names of peer reviewers — and their reviews — are published along with the journal article itself. Typically the names of reviewers are only revealed to the authors, and the public, if the article is accepted for publication. Some scientists choose to "sign" their reviews with their names as a matter of principle. In some cases, [controversially](http://bradlove.org/blog/open-review), reviewers have chosen to openly publish reviews of papers that they have recommended for rejection ([see also this commentary](https://www.pygaze.org/2019/01/what-the-kanye-west-vs-taylor-swift-of-neuroscience-tells-us-about-open-science/)).

## For-Profit Publishing

Distinct from transparency of the peer review process itself, is that of the fact that most scientific journals are published by for-profit companies. For-profit journal publishers have a unique monopoly, because any scientific article is only ever published in one journal. Thus to support the intellectual activities of professors, students, and staff, university libraries have little choice to pay whatever subscription fee the journal chooses to charge. Presently, these subscription costs comprise the vast majority of most university library budgets. For example, in the 2018-19 academic year, [Dalhousie University paid a total of over CAD$3.1 million for journal subscriptions](https://www.frdr-dfdr.ca/repo/handle/doi:10.20383/101.0187); nearly half of this was to one publisher, Elsevier/Science Direct. The cost of journal access has been recognized as a problem for universities across Canada and around the world; for example the Canadian Association of Research Libraries has described these costs as ["unsustainable"](http://www.carl-abrc.ca/wp-content/uploads/2018/02/CARL_Brief_Subscription_Costs_en.pdf). While journals certainly incur costs associated with the publishing of scientific articles, that same report notes that for-profit customers typically have profit margins of 30% or greater, meaning that roughly 1/3 of subscription costs go to making the owners and shareholders of those companies more wealthy, rather than disseminating knowledge.

Adding insult to injury, in many people's view, is the fact that this for-profit publishing industry relies on the good will, and free labour, of scientists. As noted, peer reviewers are not paid for this duty, and neither are journal editors (editorial roles do carry some prestige, which provides some additional benefit and incentive). So at the end of the day, publishers rely on the free labour of scientists at universities, but then charge those same universities high fees to access the work that their employees reviewed and edited for free. Moreover, most of the published scientific work was only possible due to research grants that covered the costs of the work — and these grants are primarily provided by governments, using money collected from taxes. In other words, the general public pays for research through their taxes, but is not able to access the results of the research because it is behind a paywall that, at best, only academic "elite" can access.

## Impact Factors

Another problem that many scientists have with for-profit publishing is that individual journals are typically concerned with their reputation and, specifically, their **impact factor**. This is a score that reflects the ratio of the number of times papers in that journal are cited, relative to the total number of papers the journal has published. More citations of an article thus reflect more impact. Scientists often consider journal impact factor when deciding where to submit their work, both because of the prestige associated with high-impact journals, and ultimately because important career-related decisions (such as hiring, promotion, and grant awards), are often based on citations. A popular metric for evaluating the impact of a scientist is the **h-index**, which reflects the number of times their work has been cited (technically, *h* is defined such that the author has published *h* papers that have each been cited *h* times). Thus scientists anticipate that if their work is accepted by a high-impact factor journal, ultimately it will improve their *h*-index.

The concerning result of chasing impact factors is that journals will base decisions about which papers to accept (or even send out for review) not only on the scientific merit of the research, but on its anticipated impact. For example, as a reviewer I have received instruction from journals such as that the paper should be "ranked in the top 15-20% in terms of significance, originality, and design and quality of data”. While beneficial for journal impact factors, this exerts problematic pressure on scientists. Firstly, decisions are made based on the *anticipated* impact of a paper, by a relatively small number of people — which may or may not accurately reflect the actual impact the paper has when published. Secondly, it can lead to long delays in solid scientific work being published, if one or more journals review the paper and decide it doesn't have sufficient potential impact, independent of its scientific rigour. Reviewers are typically given 2–4 weeks to provide a review, but are often late submitting reviews. Combined with the time it takes for an editor to find reviewers, this can mean that receiving a rejection takes a month or more (or much more, if the paper goes through more than one round of reviews before a reject decision is made). Combined with the authors finding the time to revise, reformat, and resubmit a paper to a new journal, this can mean that publication takes many months or even years after the research is completed.

## Novelty

Related, but distinct, from the issue of impact factors is that most scientific journals have "novelty" as a publication criterion. In other words, a paper that presents a replication of previously-published work is unpublishable in the vast majority of journals. This is a serious problem that has led to a **replication crisis** in many fields, including Psychology. The replication crisis is, simply put, that many published results cannot be replicated. But, this information is often not available to the scientific community, because results published in peer-reviewed journals are considered the gold standard of "truth". In fact, by one line of argument (and the title of a now-famous paper), the results of "most published research findings are false" [Ioannidis, 2005](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124). Ioannidis' argument demonstrates that chasing impact factors incentivizes journals and authors to publish results that are novel and surprising, but in general if a result is surprising, that is because it goes against what is already known. While in some cases these may be true results, the odds of this are lower than for confirmatory findings. Moreover, once such a result is published, it is both harder for anyone else to publish a replication of the methods, and even harder if the replication findings are not consistent with the published, peer-reviewed results. This is not to say that the published results in question are fraudulent, or that the authors had any ill intent in publishing them. Most scientific results are based on statistics, which rely on samples of a much larger population and estimates of the probability that the result is due to chance. This carries inherent risk that any particular result *is* due to chance, and the only real way to determine this is to replicate the experiment, ideally more than once. However, given the time and cost of research, as well as the dependence of scientific careers on publication (especially high-impact publication), scientists are much more likely to publish a result sooner, rather than wait for replication (if they can even afford to, either financially or career-wise).

Another practice that has contributed to the replicability crisis is behaviour by scientists that has both been widely condoned in the past, but that is also questionable upon reflection. One of this is [**HARKing**](https://doi.org/10.1207/s15327957pspr0203_4) {cite}`kerr_harking_1998` or hypothesizing after the results are known. This essentially entails analyzing data and then writing the results up with a hypothesis (prediction about the outcome of the experiment) that is based on the results, rather than one that was created *a priori*, before the study was conducted. Statistical testing is often described as hypothesis testing, because the interpretation of statistical results relies on estimates that a result could be obtained by chance. If the statistical result is consistent with an a priori theory, this provides additional reassurance that the result is correct — because it is consistent with a theoretical prediction. On the other hand (as noted above), surprising research results are also more likely to be false, such as the result fo a Type 1 (false positive) error. Moreover, scientific hypotheses should be disconfirmable, but it is impossible to disconfirm a result that you only "predict" after you have found it (without doing more experiments).  This is not to say there is no value in following up on unexpected results, or conducting exploratory research. However, it is critical to distinguish between results that you genuinely predicted before conducting the experiment, and those findings obtained *post hoc*.

Another questionable practice is [**p-hacking**](https://doi.org/10.1371/journal.pbio.1002106){cite}`phacking`. This is the practice of iteratively collecting and analyzing data, until a specific, desired result is obtained, and then terminating data collection. This practice is problematic because, again, *p* values used to determine statistical significance reflect probability, not ultimate truth, and so there is always a probability of error. A properly-designed study will conduct a **power analysis** prior to starting, which involves using the expected size of the experimental effect, and variance among samples, to determine how large a sample size (e.g., number of participants) to collect. If the estimates of effect size and variance are accurate, then obtaining a significant *p* value from the results of a study using the estimated sample size can be taken to be probably true. However, p-hacking virtually guarantees false results, because one is essentially playing the lottery until one gets a "big win", and then quitting. This is fine (and probably advisable) when playing roulette in Las Vegas, but it is frankly dishonest scientific practice.

## Open Publishing

The open publishing movement aims to address the issues concerns around for-profit publishing, by making peer-reviewed journal articles free for anyone to access. Prominent examples of open publishing are the [Public Library of Science](https://plos.org), [Frontiers](https://www.frontiersin.org/), and [eLife](https://elifesciences.org/). These journals are published by not-for-profit corporations, so, while reviewers and editors still perform their duties for free, they are not contributing to revenue-generating activities for a for-profit company. Since the publishing process still incurs significant costs (such as copyediting, final production, and web hosting), open journals cannot operate in a fully cost-free fashion. However, rather than charging for access to read the published works, these journals charge authors a publication fee. The logic behind this is that the authors hold research grants that pay for other aspects of conducting the research, so it is natural that the costs of disseminating the work also come from these grants. It is also important to note that there has been a significant rise in **predatory journals** — journals that emulate open journals in charging a fee for publication, and providing free access to the published results, but which are often for-profit companies, and either do not require peer review for publishing, or have very low standards for peer review. Scientists concerned with the quality of their work, and how it is perceived by their peers, quickly learn to research publishers prior to submitting manuscripts so as to avoid predatory publishers.

Open journals differ in their approach to the question of impact factor. Some journals explicitly guarantee publication of any research study that is deemed to be scientifically and ethically sound, regardless of its perceived impact. Examples of this include [*PLoS ONE*](https://journals.plos.org/plosone/s/journal-information#loc-criteria-for-publication) and [*Frontiers*](https://www.frontiersin.org/about/review-system#AcceptanceCriteria). Individual papers (and the journals, in aggregate), still report impact factors, but anticipated impact is not a criterion for publication. At the same time, some open publishers see the value in curation and have journals that are more selective in the papers they accept. For instance, while PLoS ONE accepts papers based solely on scientific merit, the same organization publishes *PLoS Biology* and *PLoS Medicine*, which consider ["works of exceptional importance"](https://journals.plos.org/plosbiology/s/what-we-publish).

Another facet of open publishing is **preprint archives**. These are freely-accessible, web-based repositories on which scientists can post copies of their manuscripts prior to peer review. This has advantages and disadvantages. One advantage is that the world is provided with earlier access to scientific results — although with the caveat that in the absence of peer review, these findings could be incorrect, misleading, or even fraudulent. Good scientists, however, have reputations to build and uphold, and so are usually disincentivized publicly posting incorrect or fraudulent material. Another advantage is that, even if the scientists ultimately choose to publish their work in a "closed" journal, the preprint is still publicly available. Many journals allow the final, accepted version of a paper to be published in an open archive, with the caveat that this must not be the version formatted by the journal. One of the earliest and most prominent of these is [arXiv](https://arxiv.org/), however there are now many others. As well, many universities have their own repositories, providing their faculty and students with n in-house platform for archiving. For example, Dalhousie University's libraries offer [DalSpace](https://dalspace.library.dal.ca).

## Preregistration

A related concept is **preregistration**. This is a practice in which scientists publish their methods prior to conducting the experiment. Preregistration can either be "embargoed" or not. **Embargoed** means that the researchers register their planned methods with some independent agency, who agrees to keep the methods private until the research is ready to publish the results. This helps researchers protect from being "scooped", having others copy their methods and beat them to publication. For example, the [Open Science Framework (OSF)](https://osf.org) is a not-for-profit organization that provides a web platform on which researchers can pre-register methods and choose to have them embargoed. The web site tracks the dates when information was entered as preregistration, and any edits that were made. Thus the researcher is in control of what information is provided, and when, but the entire history of changes provides others with documentation as to whether the end results, and methods used to obtain them, are free from HARKing and p-hacking.

The other option is to publish the methods as a "preregistered report". Some journals now allow, and even encourage this. Indeed, some journals will guarantee publication of the results of any study whose preregistered methods that journal publishes. Both the preregistered methods, and the results, still need to get through peer review, but this provides a high level of transparency around methods, preventing HARKing and p-hacking, and ensures that publication is not biased by the perceived interest or novelty of the results. Preregistration is also required in some areas of research, such as clinical trials testing the efficacy of a new drug, device, or other intervention.

## Open Methods and Data

You have likely been taught that the Methods section of a published journal article should describe the methods in sufficient detail that they can be replicated by anyone reading them. In practice, this is rarely entirely true. Methods sections gloss over many details, for reasons such as assumptions about what the reader understands, or saving space. Many journals set limits on the length of articles, and usually authors are more concerned with presenting and discussing results and theory, than with the methods. As well, an overly-detailed Methods section can be tedious to read. Indeed, as you will discover in this course, data analysis often involves extremely long and complex computer code, and the "devil is in the details". Publishing thousands of lines of Python code in a journal article is not feasible.  

At the same time, a lack of transparency in methods has contributed to the replicability crisis, and arguably restricted scientific advances in cases where researchers cannot access enough of the methods to perform a replication, or even determine what factors may have contributed to a result. For instance, many studies in psychology and neuroscience rely on presenting stimuli to participants. It is relatively rare for researchers to publish their entire stimulus set (as opposed to one or two examples), and it is not uncommon for old stimuli to simply be lost, so that even if a researcher wanted to share them with a colleague later, they cannot. Likewise, since data analysis code is complex, there is always the possibility that it contains errors — but if the actual code used to obtain a result is not available, no one can audit it to catch those errors.

For these reasons, it is increasingly common for researchers to provide greater transparency in their practice by making all stimuli, analysis code, and data publicly available. The OSF platform mentioned above is one that supports this practice. In addition to pre-registering methods, researchers can post all of their stimuli, analysis code, and data. The OSF platform can also serve as a preprint archive, hosting a copy of the resulting manuscript as well. As with methods, any material placed on OSF can be embargoed, so that researchers have control of when it becomes available to the public.

Increasingly, journals are making the decision to require that researchers make at last their data, if not their analysis code and stimuli, publicly available as a criterion for publication.  Beyond transparency and accountability, these open practices can benefit the scientific community in other ways. For example, a given data set may contain interesting information that were not the focus of the analyses performed by the researchers that collected the data. By making data publicly available, other researchers can access it and perform different analyses, or meta-analyses (analyses of results over multiple studies). Given that most research is funded by taxpayer money, this also increases the value of the public investment in science. Another benefit is education. By examining the analysis code from a published study, other researchers may learn new ways of doing things, which benefit their own research. As well, open datasets can be used as sample data in educational settings (such as this course), which allows students to practice data analysis, and has the added benefit in many cases of knowing whether their results are correct, by comparison to the published results.
